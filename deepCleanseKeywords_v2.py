# Copyright 2018 Cerebro Scholar
# generated by JE KIM
# 2019. 10. 15
"""
Additional cleansing for assigned scholarly keywords.

return clean_paper and keylist.

use:
papers_clean,keylist = additional_cleansing_for_keywords
                        (papers_clean, 
                        [keytype : colum to deep clean.(default is 'keywords_author')])
"""
import pandas as pd
import re
from printUtils import *
# from akaExtractor import *

# def remove_bracket(keylist) :
#     """Temporary remove inside bracket. 
#     TODO : consider how to use inside information filtering waste info.
#     """



def checkinside(text, p) :
    #컴마가 괄호 사이에 존재하는지 확인
    m = re.compile(r'[(].*?[)]').finditer(text)
    for i in m :
        if (i.span()[0] < p) and (p < i.span()[1]) :
            return True
    return False 

def clean_a_key(key) :
    key = key.strip().lower()
    if key.endswith('.') | key.endswith(',') : key = key[:-1]
    if key.startswith('(') & key.endswith(')') : key = key[1:-1]

    return key

def AdditionalCleanse(keys) : 
    """re-designed version."""
    clean_key = []
    for x in keys :
        splitkeys = [clean_a_key(x)]
        # print(x)

        # 하나의 키가 여러개로 더 쪼개지는 경우
        # semicolon case
        if (x.find(';')!=-1) & (re.compile(r'&.+;').search(x) == None) :
            splitkeys = [clean_a_key(each) for each in x.split(';') if re.compile(r'[a-z]+').search(each.lower()) != None]
        # , case
        # elif re.compile(r',').search(x) != None :
        #     comma_p = [i for i,c in enumerate(x) if c==',' and not checkinside(x,i)]
        #     if len(comma_p) > 0 :
        #         splitkeys = [clean_a_key(x[:comma_p[0]].strip())]
        #         for i,p in enumerate(comma_p) :
        #             if i == len(comma_p)-1 :
        #                 key = x[p+1:].strip()
        #             else :
        #                 key = x[comma_p[i]+1:comma_p[i+1]].strip()

        #             if(len(key)==0) : continue
        #             elif((key[0]=="'" and key[len(key)-1] == "'") or (key[0]=='"' and key[len(key)-1] == '"')) :
        #                 key = key[1:len(key)-1]

        #             if key.lower().strip().startswith('and ') : key = key.lower().strip()[4:]

        #             splitkeys.extend([clean_a_key(key)])
                
        # print('=>',splitkeys)
        clean_key.extend(splitkeys)

        # 괄호 존재
        
    return list(set(clean_key))




def getkeys_withAdditionalCleanse(x, keytype, keylist) :
    # print(x[keytype])
    # jj = AdditionalCleanse(x[keytype])
    # if(len(set(jj)) != len(jj)) : print('problem solving!!')
    cleanseKeys = list(set(AdditionalCleanse(x[keytype])))
    # print(cleanseKeys)

    keylist.extend(cleanseKeys)
    return cleanseKeys

# AdditionalCleanse(dataset.keywords_author[papers.p_id==np.int64(10823)].values[0])
# AKA = []
# AdditionalCleanse(['Deep learning', 'Deep Convolutional Neural Networks, (DCNNs)', 'Universal features', 'Retinal image analysis', 'Age -related macular degeneration, (AMD)', 'Transfer learning'])


def Detail_check(df, p_id=52738) :
    df.set_index('p_id', inplace=True)
    
    a = df.loc[p_id, 'keywords_author']
    print('before :', a)
    b = df.loc[p_id, 'keywords_author_moreclean']
    print('after :', b)

    if set(a)==set(b) : print('===> SAME')
    else : print('===> NOT SAME')

    return
        
def what_different(df, ex_num=5) :
    pd.set_option('display.max_colwidth', 100)

    df_notna = df[df.keywords_author.isna() == False]
    # a = df_notna.keywords_author.apply(set)
    a = df_notna.keywords_author.apply(lambda x: set([each.lower() for each in x]))
    b = df_notna.keywords_author_moreclean.apply(set)

    print(blue('{:,} papers\' keywords have been cleansed additionally.\n'.format(sum(a!=b))))
    print('Check Details...')
    print('* source variation follows')
    print(df_notna[a!=b].sources.value_counts())
    
#     print('='*10)
#     display(df_notna[a!=b].head(3))
#     display(df_notna[a!=b].loc[:,['keywords_author', 'keywords_author_moreclean']])

    print()
    print('* Check carefully the changes. Here\'s 5 examples.')
    print()
    exs = df_notna[a!=b][:ex_num].loc[:, 'p_id'].values
    for ex in exs:
        print('p_id :', ex)
        Detail_check(df.copy(), p_id=ex)
        print()
    
    return

def lower(keylist) :
    # print(keylist)
    if len(keylist) < 1 : return []
    return [each.lower() for each in keylist]

def additional_cleansing_for_keywords(papers_clean, keytype = 'keywords_author') :
    print(blue('\n=> Additional cleansing for {}...'.format(keytype)))
    print(blue('papers with {} : {:,}'.format(keytype, sum(papers_clean[keytype].isna() == False))))

    keylist = []
    newcol = keytype + '_moreclean'
    papers_clean[newcol] = papers_clean[papers_clean[keytype].isna() == False].apply(lambda x : getkeys_withAdditionalCleanse(x, keytype, keylist), axis=1)

    if 'rawkeys' in papers_clean.columns :
        papers_clean['rawkeys'] = papers_clean[(papers_clean[newcol].isna() == False) & (papers_clean.rawkeys.isna() == False)].apply(lambda x: list(set(x.rawkeys + lower(x[newcol]))), axis=1)
        papers_clean['rawkeys'] = papers_clean[(papers_clean[newcol].isna() == False) & (papers_clean.rawkeys.isna() == True)].apply(lambda x: list(set(lower(x[newcol]))), axis=1)
    else :
        papers_clean['rawkeys'] = papers_clean[papers_clean[newcol].isna() == False].apply(lambda x: lower(x[newcol]), axis=1)

    what_different(papers_clean)

    # AKA = aka_extractor(keylist)

    # keylist = remove_bracket(keylist)



    return papers_clean, keylist